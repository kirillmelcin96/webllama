# Webllama
**Webllama** - is a selfhosted offline AI platform providing access to ollama models using API. Written in Vue.js

![Demo image](https://github.com/kirillmelcin96/webllama/blob/main/demo.png)

## Key features

- ğŸš€ **Simple installation**: Just clone the repo and run webllama in your browser! No docker required

- âœˆï¸ **Working offline**

- âš™ï¸ **Auto-detect of ollama models**: If you already have a running ollama instance you are ready to go!

- ğŸ’¬ **Chat history**: Chats are stored directly in your browser

- ğŸ“„ **Markdown and code support**

## Installation 

1. Clone the repository
``` bash
git clone https://github.com/kirillmelcin96/webllama.git
```

2. Install dependencies
``` bash
npm install

# or if you prefer pnpm
pnpm install
```

3. Run build command

``` bash
npm run build

# or
pnpm build
```
Files will be available from the `/dist` directory

4. Open `/dist/index.html` file using your browser and that's it!

## Features to be added

- Search in chats
- Mobile version of the chat history
- Text-to-speech button
- Attaching files
- More themes
- System prompts and personal settings
- Temporary chats

